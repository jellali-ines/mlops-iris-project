"""
ZenML Pipeline Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Iris
End-to-end ML pipeline with ZenML
"""
import pickle
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from zenml import pipeline, step


# ============================================
# STEPS
# ============================================

@step
def data_loader(csv_path: str = 'data/raw/iris.csv') -> Tuple[np.ndarray, np.ndarray]:
    """
    ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Iris
    Load Iris dataset from CSV
    
    Returns:
        Features (X) and labels (y)
    """
    print("ğŸ“Š ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† CSV...")
    
    df = pd.read_csv(csv_path)
    X = df.drop(['target', 'species'], axis=1).values
    y = df['target'].values
    
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {X.shape[0]} Ø¹ÙŠÙ†Ø©ØŒ {X.shape[1]} Ù…Ù…ÙŠØ²Ø§ØªØŒ {len(np.unique(y))} Ø£ØµÙ†Ø§Ù")
    
    return X, y


@step
def data_splitter(
    X: np.ndarray,
    y: np.ndarray,
    test_size: float = 0.2,
    random_state: int = 42
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ train Ùˆ test
    Split data into train and test sets
    
    Returns:
        X_train, X_test, y_train, y_test
    """
    print(f"ğŸ”ª ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (test_size={test_size})...")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"âœ… Train: {X_train.shape[0]} Ø¹ÙŠÙ†Ø©ØŒ Test: {X_test.shape[0]} Ø¹ÙŠÙ†Ø©")
    
    return X_train, X_test, y_train, y_test


@step
def data_preprocessor(
    X_train: np.ndarray,
    X_test: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, StandardScaler]:
    """
    ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… StandardScaler
    Preprocess data with standardization
    
    Returns:
        Scaled training features, scaled test features, fitted scaler
    """
    print("âš™ï¸ ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("âœ… ØªÙ… ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    return X_train_scaled, X_test_scaled, scaler


@step
def model_trainer(
    X_train: np.ndarray,
    y_train: np.ndarray,
    C: float = 10.0,
    max_iter: int = 200
) -> LogisticRegression:
    """
    ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Logistic Regression
    Train ML model
    
    Returns:
        Trained model
    """
    print(f"ğŸ¤– ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (C={C}, max_iter={max_iter})...")
    
    model = LogisticRegression(
        C=C,
        solver='saga',
        max_iter=max_iter,
        random_state=42
    )
    
    model.fit(X_train, y_train)
    
    print("âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    
    return model


@step
def model_evaluator(
    model: LogisticRegression,
    X_test: np.ndarray,
    y_test: np.ndarray
) -> dict:
    """
    ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    Evaluate trained model
    
    Returns:
        Dictionary of metrics
    """
    print("ğŸ“ˆ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    
    metrics = {
        'accuracy': float(accuracy),
        'f1_score': float(f1)
    }
    
    print(f"âœ… Accuracy: {accuracy:.4f}")
    print(f"âœ… F1-Score: {f1:.4f}")
    
    return metrics


@step
def model_exporter(
    model: LogisticRegression,
    scaler: StandardScaler,
    metrics: dict,
    output_dir: str = 'models',
    version: str = 'zenml_v1'
) -> Tuple[str, str]:
    """
    Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù€ scaler
    Export trained model and scaler
    
    Returns:
        Paths to saved model and scaler
    """
    print(f"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (version: {version})...")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = output_dir / f"model_{version}.pkl"
    scaler_path = output_dir / f"scaler_{version}.pkl"
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    # Ø­ÙØ¸ Ø§Ù„Ù€ scaler
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {model_path}")
    print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù€ scaler ÙÙŠ: {scaler_path}")
    print(f"ğŸ“Š Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score']:.4f}")
    
    return str(model_path), str(scaler_path)


# ============================================
# PIPELINE
# ============================================

@pipeline(enable_cache=False)
def iris_training_pipeline(
    test_size: float = 0.2,
    random_state: int = 42,
    C: float = 10.0,
    max_iter: int = 200,
    version: str = 'zenml_v1'
):
    """
    Pipeline ÙƒØ§Ù…Ù„ Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Iris
    End-to-end training pipeline for Iris classification
    
    Steps:
    1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Load data)
    2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Split data)
    3. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Preprocess)
    4. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Train model)
    5. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Evaluate)
    6. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Export)
    """
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X, y = data_loader()
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_train, X_test, y_train, y_test = data_splitter(
        X=X,
        y=y,
        test_size=test_size,
        random_state=random_state
    )
    
    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_train_scaled, X_test_scaled, scaler = data_preprocessor(
        X_train=X_train,
        X_test=X_test
    )
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = model_trainer(
        X_train=X_train_scaled,
        y_train=y_train,
        C=C,
        max_iter=max_iter
    )
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    metrics = model_evaluator(
        model=model,
        X_test=X_test_scaled,
        y_test=y_test
    )
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model_path, scaler_path = model_exporter(
        model=model,
        scaler=scaler,
        metrics=metrics,
        version=version
    )


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ØªØ´ØºÙŠÙ„ ZenML training pipeline')
    parser.add_argument('--test-size', type=float, default=0.2, help='Ø­Ø¬Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±')
    parser.add_argument('--C', type=float, default=10.0, help='Ù…Ø¹Ø§Ù…Ù„ Regularization')
    parser.add_argument('--max-iter', type=int, default=200, help='Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª')
    parser.add_argument('--version', type=str, default='zenml_v1', help='Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸš€ ZenML Training Pipeline - MLOps Iris Classification")
    print("=" * 70)
    print(f"\nâš™ï¸ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:")
    print(f"   - Test Size: {args.test_size}")
    print(f"   - C: {args.C}")
    print(f"   - Max Iter: {args.max_iter}")
    print(f"   - Version: {args.version}")
    print("\n" + "=" * 70)
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ pipeline
    iris_training_pipeline(
        test_size=args.test_size,
        C=args.C,
        max_iter=args.max_iter,
        version=args.version
    )
    
    print("\n" + "=" * 70)
    print("âœ… Ø§ÙƒØªÙ…Ù„ Pipeline Ø¨Ù†Ø¬Ø§Ø­!")
    print("=" * 70)
    print("\nğŸ“‹ Ù„Ø¹Ø±Ø¶ runs:")
    print("   zenml pipeline runs list")
    print("\nğŸŒ Ù„ÙØªØ­ Dashboard:")
    print("   zenml up")
    print("\n" + "=" * 70)