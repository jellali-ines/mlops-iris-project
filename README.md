# MLOps Iris Classification Project ğŸŒ¸

This project demonstrates a complete MLOps lifecycle for the Iris dataset, featuring automated pipelines, versioning, and containerized deployment.

## ğŸ“ Project Structure

```text
mlops-iris-project/
â”œâ”€â”€ .dvc/                    # DVC (Data Version Control) configuration
â”œâ”€â”€ .gitlab-ci.yml           # GitLab CI/CD Pipeline configuration
â”œâ”€â”€ configs/                 # Hyperparameters and model configurations
â”œâ”€â”€ data/                    # Data storage (versioned by DVC)
â”‚   â”œâ”€â”€ raw/                 # Original, immutable data (e.g., iris.csv)
â”‚   â””â”€â”€ processed/           # Cleaned/transformed data for training
â”œâ”€â”€ docker/                  # Dockerfiles and deployment configurations
â”œâ”€â”€ docs/                    # Documentation and screenshots
â”œâ”€â”€ mlartifacts/             # MLflow artifacts (models, plots)
â”œâ”€â”€ mlruns/                  # MLflow experiment tracking data
â”œâ”€â”€ models/                  # Versioned model artifacts (.pkl files)
â”œâ”€â”€ monitoring/              # Grafana dashboards for performance tracking
â”œâ”€â”€ reports/                 # Generated reports (evaluation, analysis)
â”œâ”€â”€ scripts/                 # Utility scripts (setup, smoke tests, demos)
â”œâ”€â”€ src/                     # Source Code
â”‚   â”œâ”€â”€ data/                # Data loading and preprocessing logic
â”‚   â”œâ”€â”€ models/              # Model architecture and training logic
â”‚   â”œâ”€â”€ optimization/        # Hyperparameter tuning (Optuna)
â”‚   â”œâ”€â”€ pipelines/           # ZenML training pipelines orchestration
â”‚   â””â”€â”€ serving/             # FastAPI implementation for model serving
â”œâ”€â”€ tests/                   # Unit, integration, and performance tests
â”œâ”€â”€ Dockerfile               # Production container definition
â”œâ”€â”€ docker-compose.yml       # Orchestration for local multi-container setup
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ setup_project.sh         # Script to initialize the project structure
```

## ï¿½ Data Management

### Raw Data (`data/raw/`)
The `data/raw/` directory contains the **immutable source of truth** for the project.
- **Source**: `iris.csv` (Standard Fisher's Iris dataset).
- **Format**: CSV with 150 samples and 5 columns (sepal length, sepal width, petal length, petal width, target/species).
- **Rule**: Never modify files in this directory. Any cleaning or transformation must result in new files in `data/processed/`.

### Data Versioning (DVC)
All data files are tracked using **DVC** to avoid bloating the Git repository with large binary files.
- The `.dvc` files in the `data/` directory track the versions of the actual datasets.
- Use `dvc pull` to retrieve the data after cloning the repository.

## ï¿½ğŸ›  Prerequisites

- Python 3.9+
- Docker & Docker Compose
- DVC
- ZenML
- MLflow

## ğŸš€ Execution Guide

### 1. Setup Environment
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Post-setup
bash setup_project.sh

### 2. Data Preparation & DVC
```bash
# Download Iris data from sklearn
python src/data/load_data.py


# ØªØ«Ø¨ÙŠØª DVC
pip install dvc
# Initialize DVC
dvc init
dvc remote add -d mystorage C:\Users\user\dvc-storage
mkdir C:\Users\user\dvc-storage

# Track data with DVC
dvc add data/raw/iris.csv
git add data/raw/iris.csv.dvc data/raw/.gitignore
git commit -m "Add iris dataset with DVC tracking"
dvc push
```
```
Ø§Ù„Ø®Ø·ÙˆØ© 2.5: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† DVC
powershell# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© DVC
dvc status

# Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª DVC
dvc remote list 

### 2. Training Pipeline (ZenML)
The project uses ZenML to orchestrate the ML pipeline.
```bash
# Initialize ZenML
zenml init

# Run the training pipeline
python src/pipelines/training_pipeline.py --test-size 0.2 --C 1.0
```

### 3. Experiment Tracking (MLflow)
Track runs, metrics, and parameters locally or on a server.
```bash
mlflow ui
```

### 4. Data Versioning (DVC)
```bash
dvc pull  # Download data
dvc push  # Upload changes
```

### 5. Section 3.9 : DÃ©ploiement (Serving) ğŸš€
This project implements a professional serving layer as per academic requirements:

- **Stable Inference API**: Built with **FastAPI**, providing independent endpoints for:
  - `/predict`: Model inference.
  - `/health`: Service monitoring.
- **Docker Compose Orchestration**: Manages the full stack locally:
  - `api-v1`: Stable baseline service.
  - `api-v2` / `optuna_best`: Optimized services.
  - `mlflow`: Tracking backend.
- **v1 â†’ v2 + Rollback Simulation**: 
  - Verified via the `demo_deployment_v1_v2_rollback.py` script.
  - **Proof**: Terminal output logs success for upgrade and emergency rollback to v1.

```bash
# Run the Rollback simulation
python demo_deployment_v1_v2_rollback.py
```

### 6. Hyperparameter Optimization (Optuna)
Fine-tune model parameters using Optuna.
```bash
python src/optimization/optuna_tuning.py
```

### 7. Monitoring (Grafana)
The project includes predefined dashboards in `monitoring/grafana-dashboards/`.
- Import these dashboards into your Grafana instance to monitor model health and performance metrics.

## ğŸ”„ CI/CD Automation
The project supports dual CI/CD platforms for maximum flexibility:
1. **GitHub Actions**: Workflows are located in `.github/workflows/`. This enables the **"Actions"** tab on GitHub.
2. **GitLab CI**: Configuration is in `.gitlab-ci.yml`.

### Pipeline Stages:
1. **Test**: Linting (flake8) and Unit Tests (pytest).
2. **Build**: Automated Docker image building.
3. **Smoke Test**: 
    - **Continuous Training (CT)**: Automated quick training with a subset of data (30 samples, 10 iterations).
    - **API Health**: Verification of endpoints.

---
Developed as part of the MLOps training program.
